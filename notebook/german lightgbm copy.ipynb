{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "best_value = 0\n",
    "depth = 8\n",
    "random_state = 114\n",
    "n_estimators = 100\n",
    "data_table = pd.read_csv('data/german.csv')\n",
    "\n",
    "X = data_table.drop('Creditability', axis=1).values\n",
    "y = data_table['Creditability'].values\n",
    "features = data_table.drop('Creditability', axis=1).columns.to_list()\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=random_state)\n",
    "\n",
    "sm = SMOTE(random_state=random_state)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    num_leaves=50,\n",
    "    reg_alpha=3,\n",
    "    max_depth=3,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Test')\n",
    "print('Accuracy Score is', accuracy_score(y_test, y_pred))\n",
    "#rint('Precision is', precision_score(y_test, y_pred))\n",
    "#print('Recall is', recall_score(y_test, y_pred))\n",
    "#print('F1-Score is', f1_score(y_test, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "print('Train')\n",
    "print('Accuracy Score is', accuracy_score(y_train, y_pred))\n",
    "#print('Precision is', precision_score(y_train, y_pred))\n",
    "#print('Recall is', recall_score(y_train, y_pred))\n",
    "#print('F1-Score is', f1_score(y_train, y_pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test\n",
      "Accuracy Score is 0.8133333333333334\n",
      "Train\n",
      "Accuracy Score is 0.8636363636363636\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from os import path\n",
    "from copy import deepcopy\n",
    "\n",
    "def visit_boosting_tree(tree, path = {}):\n",
    "    if 'decision_type' not in tree:\n",
    "        path['value'] = tree['leaf_value']\n",
    "        path['weight'] = tree['leaf_weight']\n",
    "        return [path]\n",
    "    \n",
    "    key = tree['split_feature']\n",
    "    thres = tree['threshold']\n",
    "    ret = []\n",
    "    leftpath = deepcopy(path)\n",
    "    if key in leftpath:\n",
    "        r = leftpath[key]\n",
    "        leftpath[key] = [r[0], min(r[1], thres)]\n",
    "    else:\n",
    "        leftpath[key] = [-1e9, thres]\n",
    "    ret += visit_boosting_tree(tree['left_child'], leftpath)\n",
    "\n",
    "    rightpath = deepcopy(path)\n",
    "    if key in rightpath:\n",
    "        r = rightpath[key]\n",
    "        rightpath[key] = [max(r[0], thres), r[1]]\n",
    "    else:\n",
    "        rightpath[key] = [thres, 1e9]\n",
    "    ret += visit_boosting_tree(tree['right_child'], rightpath)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def visit_decision_tree(tree, index = 0, path = {}):\n",
    "    if tree.children_left[index] == -1 and tree.children_right[index] == -1:\n",
    "        return [path]\n",
    "    key = tree.feature[index]\n",
    "    thres = tree.threshold[index]\n",
    "    ret = []\n",
    "    leftpath = deepcopy(path)\n",
    "    if key in leftpath:\n",
    "        r = leftpath[key]\n",
    "        leftpath[key] = [r[0], min(r[1], thres)]\n",
    "    else:\n",
    "        leftpath[key] = [-1e9, thres]\n",
    "    ret += visit_decision_tree(tree, tree.children_left[index], leftpath)\n",
    "    \n",
    "    rightpath = deepcopy(path)\n",
    "    if key in rightpath:\n",
    "        r = rightpath[key]\n",
    "        rightpath[key] = [max(r[0], thres), r[1]]\n",
    "    else:\n",
    "        rightpath[key] = [thres, 1e9]\n",
    "    ret += visit_decision_tree(tree, tree.children_right[index], rightpath)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def path_extractor(model, model_type):\n",
    "    if model_type == 'random forest' :\n",
    "        ret = []\n",
    "        for estimator in model.estimators_:\n",
    "            ret += path_extractor(estimator, 'decision tree')\n",
    "        return ret\n",
    "    elif model_type == 'lightgbm':\n",
    "        ret = []\n",
    "        info = model._Booster.dump_model()\n",
    "        for tree in info['tree_info']:\n",
    "            ret += visit_boosting_tree(tree['tree_structure'])\n",
    "        return ret\n",
    "    elif model_type == 'decision tree':\n",
    "        return visit_decision_tree(model.tree_)\n",
    "    return []\n",
    "    \n",
    "paths = path_extractor(r_clf, 'lightgbm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "info = r_clf._Booster.dump_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "info['tree_info'][0]['tree_structure']['left_child']['left_child']['left_child']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'leaf_index': 0,\n",
       " 'leaf_value': -1.46578154060707,\n",
       " 'leaf_weight': 9.749999999999998,\n",
       " 'leaf_count': 39}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class PathExtractor():\n",
    "    def __init__(self, forest, X, y, X_train, y_train, features):\n",
    "        self.X, self.y = X, y   # original training data\n",
    "\n",
    "        self.features = features\n",
    "        self.n_features = len(self.features)\n",
    "        self.categories = np.unique(y).tolist()\n",
    "        self.n_categories = len(self.categories)\n",
    "        self.feature_range = [np.min(X, axis=0), np.max(X, axis=0)+1e-9]\n",
    "        self.category_total = [np.sum(self.y == i)\n",
    "                               for i in range(self.n_categories)]\n",
    "        self.forest = forest\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.n_examples = len(self.y)\n",
    "        self.n_examples2 = len(self.y_train)\n",
    "        self.n_estimators = forest.n_estimators\n",
    "\n",
    "    def get_paths(self, min_impurity_decrease=0.0):\n",
    "        self.paths = [[] for i in range(self.n_estimators)]\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        for i in range(self.n_estimators):\n",
    "            self.dfs(i, 0, {}, np.ones(self.n_examples),\n",
    "                     np.ones(self.n_examples2))\n",
    "        return self.paths\n",
    "\n",
    "    def dfs(self, i, u, feature_range, vec_examples, vec_examples2):\n",
    "        tr = self.forest.estimators_[i].tree_\n",
    "\n",
    "        def impurity_decrease(tr, u):\n",
    "            N_t = tr.n_node_samples[u]\n",
    "            I_t = tr.impurity[u]\n",
    "            N = tr.n_node_samples[0]\n",
    "            Lc = tr.children_left[u]\n",
    "            Rc = tr.children_right[u]\n",
    "            N_l = tr.n_node_samples[Lc]\n",
    "            I_l = tr.impurity[Lc]\n",
    "            N_r = tr.n_node_samples[Rc]\n",
    "            I_r = tr.impurity[Rc]\n",
    "            return N_t/N*(I_t-N_r/N_t*I_r-N_l/N_t*I_l)\n",
    "\n",
    "        def cpy(m):\n",
    "            return {key: m[key].copy() for key in m}\n",
    "\n",
    "        if tr.children_left[u] < 0 or tr.children_right[u] < 0 or impurity_decrease(tr, u) < self.min_impurity_decrease:\n",
    "            distribution = [np.dot(vec_examples, self.y == cid)\n",
    "                            for cid in range(self.n_categories)]\n",
    "            distribution2 = [np.dot(vec_examples2, self.y_train == cid)\n",
    "                             for cid in range(self.n_categories)]\n",
    "            output = np.argmax(distribution2)\n",
    "            coverage = sum(distribution)\n",
    "            if coverage > 0:\n",
    "                self.paths[i].append({\n",
    "                    \"name\": 'r%d-%d' % (len(self.paths[i]), i),\n",
    "                    \"tree_index\": i,\n",
    "                    \"rule_index\": len(self.paths[i]),\n",
    "                    \"range\": {str(key): feature_range[key].copy() for key in feature_range},\n",
    "                    \"distribution\": distribution,\n",
    "                    \"coverage\": coverage,\n",
    "                    \"fidelity\": distribution[int(output)] / coverage,\n",
    "                    \"sample\": vec_examples,\n",
    "                    \"output\": str(output)\n",
    "                })\n",
    "        else:\n",
    "            feature = tr.feature[u]\n",
    "            threshold = tr.threshold[u]\n",
    "\n",
    "            _feature_range = cpy(feature_range)\n",
    "            if not feature in feature_range:\n",
    "                _feature_range[feature] = [self.feature_range[0]\n",
    "                                           [feature], self.feature_range[1][feature]+1e-9]\n",
    "            _feature_range[feature][1] = min(\n",
    "                _feature_range[feature][1], threshold)\n",
    "\n",
    "            _vec_examples = vec_examples*(self.X[:, feature] <= threshold)\n",
    "            _vec_examples2 = vec_examples2 * \\\n",
    "                (self.X_train[:, feature] <= threshold)\n",
    "\n",
    "            self.dfs(\n",
    "                i, tr.children_left[u], _feature_range, _vec_examples, _vec_examples2)\n",
    "\n",
    "            _feature_range = cpy(feature_range)\n",
    "            if not feature in feature_range:\n",
    "                _feature_range[feature] = [self.feature_range[0]\n",
    "                                           [feature], self.feature_range[1][feature]]\n",
    "            _feature_range[feature][0] = threshold\n",
    "\n",
    "            _vec_examples = vec_examples*(self.X[:, feature] > threshold)\n",
    "            _vec_examples2 = vec_examples2 * \\\n",
    "                (self.X_train[:, feature] > threshold)\n",
    "            self.dfs(\n",
    "                i, tr.children_right[u], _feature_range, _vec_examples, _vec_examples2)\n",
    "\n",
    "    # given X as input, find the range of fid-th feature to keep the prediction unchanged\n",
    "    def getRange(self, X, fid):\n",
    "        step = (self.feature_range[1][fid]-self.feature_range[0][fid])*0.005\n",
    "        L, R = X[fid], X[fid]\n",
    "        Xi = X.copy()\n",
    "        ei = np.array([1 if i == fid else 0 for i in range(self.n_features)])\n",
    "        result0 = self.predict([X])[0]\n",
    "        result1 = result0\n",
    "\n",
    "        while(result1 == result0 and L > self.feature_range[0][fid]):\n",
    "            Xi = Xi-step*ei\n",
    "            result1 = self.predict([Xi])[0]\n",
    "            L -= step\n",
    "        L = max(L, self.feature_range[0][fid])\n",
    "        LC = result1\n",
    "\n",
    "        Xi = X.copy()\n",
    "        while(result1 == result0 and R < self.feature_range[1][fid]):\n",
    "            Xi = Xi+step*ei\n",
    "            result1 = self.predict([Xi])[0]\n",
    "            R += step\n",
    "        R = min(R, self.feature_range[1][fid])\n",
    "        RC = result1\n",
    "        return {\n",
    "            \"L\": L,\n",
    "            \"LC\": LC,  # the prediction when X[fid]=L-eps\n",
    "            \"R\": R,\n",
    "            \"RC\": RC,  # the prediction when X[fid]=R+eps\n",
    "        }\n",
    "\n",
    "rf = PathExtractor(r_clf, X, y, X_train, y_train, features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import pulp\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "class Extractor:\n",
    "    # 可以调用的接口：compute_accuracy和extract\n",
    "    def __init__(self, rf, X_train, y_train, X_test, y_test):\n",
    "        # rf：随机森林模型\n",
    "        # X_raw、y_raw：训练数据集\n",
    "        self.rf = rf\n",
    "        self.X_raw = X_train\n",
    "        self.y_raw = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        _paths = rf.get_paths()\n",
    "        self.paths = [p for r in _paths for p in r if p['fidelity'] > 0.7]\n",
    "        self.paths.sort(key=lambda x: -x['coverage'])\n",
    "\n",
    "    def compute_accuracy_on_train(self, paths):\n",
    "        # 计算数据集在给定规则集下的accuracy\n",
    "        # paths：规则集，为list\n",
    "        Mat = self.getMat(self.X_raw, self.y_raw, paths)\n",
    "        idx = np.argwhere(np.all(Mat[..., :] == 0, axis=0))\n",
    "        Mat = np.delete(Mat, idx, axis=1)\n",
    "        right = np.sum(Mat, axis=0)\n",
    "        return np.sum(np.where(right >= 0, 1, 0)) / len(self.X_raw)\n",
    "\n",
    "    def compute_accuracy_on_test(self, paths):\n",
    "        # 计算数据集在给定规则集下的accuracy\n",
    "        # paths：规则集，为list\n",
    "        Mat = self.getMat(self.X_test, self.y_test, paths)\n",
    "        idx = np.argwhere(np.all(Mat[..., :] == 0, axis=0))\n",
    "        Mat = np.delete(Mat, idx, axis=1)\n",
    "        right = np.sum(Mat, axis=0)\n",
    "        return np.sum(np.where(right >= 0, 1, 0)) / len(self.X_test)\n",
    "\n",
    "    def extract(self, max_num, tau):\n",
    "        # 根据给定的max_num和tau，使用rf的全部规则和数据集抽取出相应的规则\n",
    "        # max_num：抽取出规则的最大数量\n",
    "        # tau：每个样本允许的最大惩罚\n",
    "        # 返回抽取出规则的列表、数据集使用全部规则的accuracy、数据集使用抽取规则的accuracy\n",
    "        Mat = self.getMat(self.X_raw, self.y_raw, self.paths)\n",
    "        w = self.getWeight(Mat)\n",
    "        new_paths, new_path_indexes = self.LP_extraction(w, Mat, max_num, tau)\n",
    "        accuracy_origin = self.compute_accuracy_on_test(self.paths)\n",
    "        accuracy_new = self.compute_accuracy_on_test(new_paths)\n",
    "        return new_path_indexes, new_paths, accuracy_origin, accuracy_new\n",
    "\n",
    "    def path_score(self, path, X, y):\n",
    "        ans = 2 * (y == int(path.get('output'))) - 1\n",
    "        m = path.get('range')\n",
    "        for key in m:\n",
    "            ans = ans * (X[:, int(key)] >= m[key][0]) * (X[:, int(key)] < m[key][1])\n",
    "        return ans\n",
    "\n",
    "    def getMat(self, X_raw, y_raw, paths):\n",
    "        # 覆盖矩阵Mat\n",
    "        Mat = np.array([self.path_score(p, X_raw, y_raw) for p in paths]).astype('float')\n",
    "        return Mat\n",
    "\n",
    "    def getWeight(self, Mat):\n",
    "        # 权重向量w\n",
    "        RXMat = np.abs(Mat)\n",
    "        XRMat = RXMat.transpose()\n",
    "        XXAnd = np.dot(XRMat, RXMat)\n",
    "        XROne = np.ones(XRMat.shape)\n",
    "        XXOr = 2 * np.dot(XROne, RXMat) - XXAnd\n",
    "        XXOr = (XXOr + XXOr.transpose()) / 2\n",
    "        XXDis = 1 - XXAnd / XXOr\n",
    "        K = int(np.ceil(np.sqrt(len(self.X_raw))))\n",
    "        clf = LocalOutlierFactor(n_neighbors=K, metric=\"precomputed\")\n",
    "        clf.fit(XXDis)\n",
    "        XW = -clf.negative_outlier_factor_\n",
    "        MXW, mXW = np.max(XW), np.min(XW)\n",
    "        XW = 1 + (3 - 1) * (XW - mXW) / (MXW - mXW)\n",
    "        return XW / np.sum(XW)\n",
    "\n",
    "    def LP_extraction(self, w, Mat, max_num, tau):\n",
    "        m = pulp.LpProblem(sense=pulp.LpMinimize)\n",
    "        # 创建最小化问题\n",
    "        var = []\n",
    "        for i in range(len(self.paths)):\n",
    "            var.append(pulp.LpVariable(f'x{i}', cat=pulp.LpContinuous, lowBound=0, upBound=1))\n",
    "        for i in range(len(w)):\n",
    "            var.append(pulp.LpVariable(f'k{i}', cat=pulp.LpContinuous, lowBound=0))\n",
    "        # 添加变量x_0至x_{M-1}, k_0至k_{N-1}\n",
    "\n",
    "        m += pulp.lpSum([w[j] * (var[j + len(self.paths)])\n",
    "                         for j in range(len(w))])\n",
    "        # 添加目标函数\n",
    "\n",
    "        m += (pulp.lpSum([var[j] for j in range(len(self.paths))]) <= max_num)\n",
    "        # 筛选出不超过max_num条规则\n",
    "\n",
    "        for j in range(len(w)):\n",
    "            m += (var[j + len(self.paths)] >= 1000 + tau - pulp.lpSum([var[k] * Mat[k][j] for k in range(len(self.paths))]))\n",
    "            m += (var[j + len(self.paths)] >= 1000)\n",
    "            # max约束\n",
    "\n",
    "        m.solve(pulp.PULP_CBC_CMD())#solver = pulp.solver.CPLEX())#\n",
    "        new_paths = [self.paths[i] for i in range(len(self.paths)) if var[i].value() > 0]\n",
    "        new_path_indexes = [self.paths[i]['name'] for i in range(len(self.paths)) if var[i].value() > 0.5]\n",
    "        return new_paths, new_path_indexes\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "ex = Extractor(rf, X_train, y_train, X_test, y_test)\n",
    "ret = ex.extract(50, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "print(ret[2:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.8, 0.8133333333333334)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "paths = rf.get_paths()\n",
    "\n",
    "all_paths = []\n",
    "for t in paths:\n",
    "    all_paths = all_paths + t"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1337d84936ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_paths\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mget_paths\u001b[0;34m(self, min_impurity_decrease)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_impurity_decrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             self.dfs(i, 0, {}, np.ones(self.n_examples),\n\u001b[0m\u001b[1;32m     62\u001b[0m                      np.ones(self.n_examples2))\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(self, i, u, feature_range, vec_examples, vec_examples2)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.dfs(\n\u001b[0m\u001b[1;32m    118\u001b[0m                 i, tr.children_left[u], _feature_range, _vec_examples, _vec_examples2)\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(self, i, u, feature_range, vec_examples, vec_examples2)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.dfs(\n\u001b[0m\u001b[1;32m    118\u001b[0m                 i, tr.children_left[u], _feature_range, _vec_examples, _vec_examples2)\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(self, i, u, feature_range, vec_examples, vec_examples2)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.dfs(\n\u001b[0m\u001b[1;32m    118\u001b[0m                 i, tr.children_left[u], _feature_range, _vec_examples, _vec_examples2)\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(self, i, u, feature_range, vec_examples, vec_examples2)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.dfs(\n\u001b[0m\u001b[1;32m    118\u001b[0m                 i, tr.children_left[u], _feature_range, _vec_examples, _vec_examples2)\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(self, i, u, feature_range, vec_examples, vec_examples2)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0m_vec_examples2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_examples2\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             self.dfs(\n\u001b[0m\u001b[1;32m    130\u001b[0m                 i, tr.children_right[u], _feature_range, _vec_examples, _vec_examples2)\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(self, i, u, feature_range, vec_examples, vec_examples2)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_right\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mimpurity_decrease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             distribution = [np.dot(vec_examples, self.y == cid)\n\u001b[1;32m     85\u001b[0m                             for cid in range(self.n_categories)]\n",
      "\u001b[0;32m<ipython-input-4-538815d23862>\u001b[0m in \u001b[0;36mimpurity_decrease\u001b[0;34m(tr, u)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mN_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_node_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mI_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpurity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN_t\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI_t\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN_r\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN_t\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mI_r\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN_l\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN_t\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mI_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(r_clf)\n",
    "shap_values = explainer(X)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\n",
    "features=[\n",
    "    {\n",
    "        \"name\": rf.features[i],\n",
    "        \"lbound\":rf.feature_range[0][i],\n",
    "        \"rbound\":rf.feature_range[1][i],\n",
    "        \"importance\":r_clf.feature_importances_[i],\n",
    "        \"options\":\"+\",\n",
    "    } for i in range(rf.n_features)\n",
    "]\n",
    "\n",
    "data = {\n",
    "    'paths': all_paths,\n",
    "    'features': features,\n",
    "    'selected': ret[0],\n",
    "    'shap_values': shap_values,\n",
    "}\n",
    "\n",
    "import pickle\n",
    "pickle.dump(data, open('output/german.pkl', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "print(shap_values[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".values =\n",
      "array([[ 1.09110125e-01, -1.09110125e-01],\n",
      "       [ 3.01264698e-03, -3.01264698e-03],\n",
      "       [-1.24874332e-01,  1.24874332e-01],\n",
      "       [ 2.10891519e-03, -2.10891519e-03],\n",
      "       [-1.99799022e-02,  1.99799022e-02],\n",
      "       [ 3.40283799e-02, -3.40283799e-02],\n",
      "       [ 3.85628726e-02, -3.85628726e-02],\n",
      "       [ 5.60976830e-03, -5.60976830e-03],\n",
      "       [ 3.36153339e-02, -3.36153339e-02],\n",
      "       [ 3.85308858e-03, -3.85308858e-03],\n",
      "       [-2.39386825e-02,  2.39386825e-02],\n",
      "       [ 7.02421386e-03, -7.02421386e-03],\n",
      "       [ 1.96232689e-02, -1.96232689e-02],\n",
      "       [-1.74483971e-02,  1.74483971e-02],\n",
      "       [ 6.65478065e-02, -6.65478065e-02],\n",
      "       [ 6.73321595e-03, -6.73321595e-03],\n",
      "       [ 1.58162875e-03, -1.58162875e-03],\n",
      "       [-6.93273863e-05,  6.93273863e-05],\n",
      "       [ 5.12965848e-03, -5.12965848e-03],\n",
      "       [ 2.51444887e-03, -2.51444887e-03]])\n",
      "\n",
      ".base_values =\n",
      "array([0.50028956, 0.49971044])\n",
      "\n",
      ".data =\n",
      "array([   1,   18,    4,    2, 1049,    1,    2,    4,    2,    1,    4,\n",
      "          2,   21,    3,    1,    1,    3,    1,    1,    1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "ret[0][3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'r99-69'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "data = pd.read_csv('data/german.data', sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "data[data.columns[3]].dtype == 'O'"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit"
  },
  "interpreter": {
   "hash": "e4ca62cc624854f73843cd7b3352ae633eb01f3e4f77eee16509c1692ddd1ed1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}