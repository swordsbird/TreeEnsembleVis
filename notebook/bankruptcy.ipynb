{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "#import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "random_state = 114\n",
    "criterion = \"entropy\"\n",
    "max_depth = 8\n",
    "max_features = \"sqrt\"\n",
    "n_estimators = 150\n",
    "max_leaf_nodes = 90\n",
    "bootstrap = True\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "data_table = pd.read_csv('data/german.csv')\n",
    "\n",
    "X = data_table.drop('Creditability', axis=1).values\n",
    "y = data_table['Creditability'].values\n",
    "features = data_table.drop('Creditability', axis=1).columns.to_list()\n",
    "\n",
    "data_table = pd.read_csv('data/cancer.csv')\n",
    "\n",
    "X = data_table.drop('diagnosis', axis=1).values\n",
    "y = data_table['diagnosis'].values\n",
    "features = data_table.drop('diagnosis', axis=1).columns.to_list()\n",
    "\n",
    "'''\n",
    "data_table = pd.read_csv('data/bank.csv')\n",
    "\n",
    "X = data_table.drop('Bankrupt?', axis=1).values\n",
    "y = data_table['Bankrupt?'].values\n",
    "features = data_table.drop('Bankrupt?', axis=1).columns.to_list()\n",
    "\n",
    "sm = SMOTE(random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=random_state)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    num_leaves=50,\n",
    "    reg_alpha=3,\n",
    "    max_depth=8,\n",
    "    random_state=random_state,\n",
    ")\n",
    "'''\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    max_leaf_nodes=max_leaf_nodes,\n",
    "    max_features=max_features,\n",
    "    bootstrap=bootstrap,\n",
    "    criterion=criterion, \n",
    "    max_depth=max_depth,\n",
    "    random_state=random_state,\n",
    "    n_estimators=n_estimators,\n",
    ")\n",
    "'''\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Test')\n",
    "print((y_test == y_pred).sum() / len(y_test))\n",
    "print('Accuracy Score is', accuracy_score(y_test, y_pred))\n",
    "print('Precision is', precision_score(y_test, y_pred))\n",
    "print('Recall is', recall_score(y_test, y_pred))\n",
    "print('F1-Score is', f1_score(y_test, y_pred))\n",
    "\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "print('Train')\n",
    "print((y_train == y_pred).sum() / len(y_train))\n",
    "print('Accuracy Score is', accuracy_score(y_train, y_pred))\n",
    "print('Precision is', precision_score(y_train, y_pred))\n",
    "print('Recall is', recall_score(y_train, y_pred))\n",
    "print('F1-Score is', f1_score(y_train, y_pred))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test\n",
      "0.9672531769305963\n",
      "Accuracy Score is 0.9672531769305963\n",
      "Precision is 0.4714285714285714\n",
      "Recall is 0.5238095238095238\n",
      "F1-Score is 0.4962406015037594\n",
      "Train\n",
      "0.9998916811091855\n",
      "Accuracy Score is 0.9998916811091855\n",
      "Precision is 0.9997834091401343\n",
      "Recall is 1.0\n",
      "F1-Score is 0.9998916928408967\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from tree_extractor import path_extractor\n",
    "\n",
    "paths = path_extractor(clf, 'lightgbm', (X_train, y_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(paths[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'range': {23: [-1000000000.0, 105.40000000000002], 28: [-1000000000.0, 0.12195000000000002]}, 'value': 0.1891891891891892, 'weight': 55.5, 'confidence': 1, 'tree': 0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pulp\n",
    "from copy import deepcopy\n",
    "\n",
    "class Extractor:\n",
    "    # 可以调用的接口：compute_accuracy和extract\n",
    "    def __init__(self, paths, X_train, y_train, X_test, y_test):\n",
    "        # X_raw、y_raw：训练数据集\n",
    "        self.X_raw = X_train\n",
    "        self.y_raw = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.paths = [path for path in paths if path['confidence'] > 0.7]\n",
    "        print('paths', len(paths))\n",
    "\n",
    "    def compute_accuracy_on_train(self, paths):\n",
    "        # 计算训练集在给定规则集下的accuracy\n",
    "        # paths：规则集\n",
    "        y_pred = self.predict(self.X_raw, paths)\n",
    "        y_pred = np.where(y_pred == 1, 1, 0)\n",
    "        return np.sum(np.where(y_pred == self.y_raw, 1, 0)) / len(self.X_raw)\n",
    "\n",
    "    def compute_accuracy_on_test(self, paths):\n",
    "        # 计算测试集在给定规则集下的accuracy\n",
    "        # paths：规则集\n",
    "        y_pred = self.predict(self.X_test, paths)\n",
    "        y_pred = np.where(y_pred == 1, 1, 0)\n",
    "        return np.sum(np.where(y_pred == self.y_test, 1, 0)) / len(self.X_test)\n",
    "\n",
    "    def extract(self, max_num, tau):\n",
    "        # 根据给定的max_num和tau，使用rf的全部规则和数据集抽取出相应的规则\n",
    "        # max_num：抽取出规则的最大数量\n",
    "        # tau：每个样本允许的最大惩罚\n",
    "        # 返回抽取出规则的列表、数据集使用全部规则的accuracy、数据集使用抽取规则的accuracy\n",
    "        Mat = self.getMat(self.X_raw, self.y_raw, self.paths)\n",
    "        print('getWeight')\n",
    "        w = self.getWeight(self.getMat(self.X_raw, self.y_raw, self.paths))\n",
    "        print('LP_extraction')\n",
    "        paths_weight = self.LP_extraction(w, Mat, max_num, tau)\n",
    "        print('compute_accuracy_on_test')\n",
    "        accuracy_origin = self.compute_accuracy_on_test(self.paths)\n",
    "        accuracy_origin1 = self.compute_accuracy_on_train(self.paths)\n",
    "        path_copy = deepcopy(self.paths)\n",
    "        for i in range(len(path_copy)):\n",
    "            path_copy[i]['weight'] = paths_weight[i]\n",
    "        accuracy_new = self.compute_accuracy_on_test(path_copy)\n",
    "        accuracy_new1 = self.compute_accuracy_on_train(path_copy)\n",
    "        return paths_weight, accuracy_origin1, accuracy_new1, accuracy_origin, accuracy_new\n",
    "\n",
    "    def predict(self, X, paths):\n",
    "        # 根据给定规则集对数据进行预测\n",
    "        Y = np.zeros(X.shape[0])\n",
    "        for p in paths:\n",
    "            ans = np.ones(X.shape[0])\n",
    "            m = p.get('range')\n",
    "            for key in m:\n",
    "                ans = ans * (X[:,int(key)] >= m[key][0]) * (X[:,int(key)] < m[key][1])\n",
    "            Y += ans * (p.get('weight') * p.get('value'))\n",
    "        Y = np.where(Y > 0, 1, 0)\n",
    "        return Y\n",
    "\n",
    "    def getMat(self, X_raw, y_raw, paths):\n",
    "        Mat = np.array([self.path_score(p, X_raw, y_raw) for p in paths]).astype('float')\n",
    "        return Mat\n",
    "\n",
    "    def path_score(self, path, X, y):\n",
    "        value = float(path.get('value'))\n",
    "        ans = 2 * (value * y > 0) - 1\n",
    "        m = path.get('range')\n",
    "        for key in m:\n",
    "            ans = ans * (X[:, int(key)] >= m[key][0]) * (X[:, int(key)] < m[key][1])\n",
    "        return ans\n",
    "\n",
    "    def getWeight(self, Mat):\n",
    "        # 权重向量w\n",
    "        RXMat = np.abs(Mat)\n",
    "        XRMat = RXMat.transpose()\n",
    "        XXAnd = np.dot(XRMat, RXMat)\n",
    "        XROne = np.ones(XRMat.shape)\n",
    "        XXOr = 2 * np.dot(XROne, RXMat) - XXAnd\n",
    "        XXOr = (XXOr + XXOr.transpose()) / 2\n",
    "        XXDis = 1 - XXAnd / XXOr\n",
    "        K = int(np.ceil(np.sqrt(len(self.X_raw))))\n",
    "        clf = LocalOutlierFactor(n_neighbors=K, metric=\"precomputed\")\n",
    "        clf.fit(XXDis)\n",
    "        XW = -clf.negative_outlier_factor_\n",
    "        MXW, mXW = np.max(XW), np.min(XW)\n",
    "        XW = 1 + (3 - 1) * (XW - mXW) / (MXW - mXW)\n",
    "        return XW / np.sum(XW)\n",
    "\n",
    "    def LP_extraction(self, w, Mat, max_num, tau):\n",
    "        m = pulp.LpProblem(sense=pulp.LpMinimize)\n",
    "        # 创建最小化问题\n",
    "        var = []\n",
    "        for i in range(len(self.paths)):\n",
    "            var.append(pulp.LpVariable(f'x{i}', cat=pulp.LpContinuous, lowBound=0, upBound=1))\n",
    "        for i in range(len(w)):\n",
    "            var.append(pulp.LpVariable(f'k{i}', cat=pulp.LpContinuous, lowBound=0))\n",
    "        # 添加变量x_0至x_{M-1}, k_0至k_{N-1}\n",
    "\n",
    "        m += pulp.lpSum([w[j] * (var[j + len(self.paths)])\n",
    "                         for j in range(len(w))])\n",
    "        # 添加目标函数\n",
    "\n",
    "        m += (pulp.lpSum([var[j] for j in range(len(self.paths))]) <= max_num)\n",
    "        # 筛选出不超过max_num条规则\n",
    "\n",
    "        for j in range(len(w)):\n",
    "            m += (var[j + len(self.paths)] >= 1000 + tau - pulp.lpSum(\n",
    "                [var[k] * Mat[k][j] for k in range(len(self.paths))]))\n",
    "            m += (var[j + len(self.paths)] >= 1000)\n",
    "            # max约束\n",
    "\n",
    "        m.solve(pulp.PULP_CBC_CMD())  # solver = pulp.solver.CPLEX())#\n",
    "        paths_weight = [var[i].value() for i in range(len(self.paths))]\n",
    "        paths_weight = np.array(paths_weight)\n",
    "        paths_weight = paths_weight / np.sum(paths_weight)\n",
    "        for k in np.argsort(paths_weight)[:-max_num]:\n",
    "            paths_weight[k] = 0\n",
    "        print('paths_weight', sum(paths_weight))\n",
    "        return paths_weight\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "ex = Extractor(paths, X_train, y_train, X_test, y_test)\n",
    "ret = ex.extract(50, 2)\n",
    "print(ret[1:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "paths 4460\n",
      "getWeight\n",
      "LP_extraction\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.9018595041322314, 0.9948347107438017, 0.8, 0.7566666666666667)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(968, 20)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "print(ret[2:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.7733333333333333, 0.62)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(r_clf)\n",
    "shap_values = explainer(X)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\n",
    "features=[\n",
    "    {\n",
    "        \"name\": rf.features[i],\n",
    "        \"lbound\":rf.feature_range[0][i],\n",
    "        \"rbound\":rf.feature_range[1][i],\n",
    "        \"importance\":r_clf.feature_importances_[i],\n",
    "        \"options\":\"+\",\n",
    "    } for i in range(rf.n_features)\n",
    "]\n",
    "\n",
    "data = {\n",
    "    'paths': all_paths,\n",
    "    'features': features,\n",
    "    'selected': ret[0],\n",
    "    'shap_values': shap_values,\n",
    "}\n",
    "\n",
    "import pickle\n",
    "pickle.dump(data, open('output/german.pkl', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "print(shap_values[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".values =\n",
      "array([[ 1.09110125e-01, -1.09110125e-01],\n",
      "       [ 3.01264698e-03, -3.01264698e-03],\n",
      "       [-1.24874332e-01,  1.24874332e-01],\n",
      "       [ 2.10891519e-03, -2.10891519e-03],\n",
      "       [-1.99799022e-02,  1.99799022e-02],\n",
      "       [ 3.40283799e-02, -3.40283799e-02],\n",
      "       [ 3.85628726e-02, -3.85628726e-02],\n",
      "       [ 5.60976830e-03, -5.60976830e-03],\n",
      "       [ 3.36153339e-02, -3.36153339e-02],\n",
      "       [ 3.85308858e-03, -3.85308858e-03],\n",
      "       [-2.39386825e-02,  2.39386825e-02],\n",
      "       [ 7.02421386e-03, -7.02421386e-03],\n",
      "       [ 1.96232689e-02, -1.96232689e-02],\n",
      "       [-1.74483971e-02,  1.74483971e-02],\n",
      "       [ 6.65478065e-02, -6.65478065e-02],\n",
      "       [ 6.73321595e-03, -6.73321595e-03],\n",
      "       [ 1.58162875e-03, -1.58162875e-03],\n",
      "       [-6.93273863e-05,  6.93273863e-05],\n",
      "       [ 5.12965848e-03, -5.12965848e-03],\n",
      "       [ 2.51444887e-03, -2.51444887e-03]])\n",
      "\n",
      ".base_values =\n",
      "array([0.50028956, 0.49971044])\n",
      "\n",
      ".data =\n",
      "array([   1,   18,    4,    2, 1049,    1,    2,    4,    2,    1,    4,\n",
      "          2,   21,    3,    1,    1,    3,    1,    1,    1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "ret[0][3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'r99-69'"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "data = pd.read_csv('data/german.data', sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "data[data.columns[3]].dtype == 'O'"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit"
  },
  "interpreter": {
   "hash": "e4ca62cc624854f73843cd7b3352ae633eb01f3e4f77eee16509c1692ddd1ed1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}